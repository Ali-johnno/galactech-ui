{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final RNNT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries"
      ],
      "metadata": {
        "id": "RfSBQev81IhL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2KypAuRTCZn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1341951e-8660-4bf0-ec34-758f48183ee6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import matplotlib.pyplot as mpl \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "#import torch\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import InputLayer\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras import callbacks\n",
        "\n",
        "#!pip install librosa\n",
        "import librosa as lb\n",
        "import librosa.display\n",
        "\n",
        "import random\n",
        "from torch.utils.data import random_split\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import signal\n",
        "\n",
        "tf.__version__\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration Variables"
      ],
      "metadata": {
        "id": "rTq87YwbiXpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting seed\n"
      ],
      "metadata": {
        "id": "CYcH880_5u3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(101)\n",
        "np.random.seed(102) #does not need to be same number as tf seed"
      ],
      "metadata": {
        "id": "3YZ2kKW2Zjp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the variables used in creating the encoder, predictor, joiner and the final RNN-T Model."
      ],
      "metadata": {
        "id": "KOlD8OS38Dga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_hidden_encoder=512 # number of cells in the lstm hidden layers \n",
        "num_hidden_joiner=64  #number of cells in the dense layer of joiner\n",
        "num_hidden_predictor=64 #number of cells in the lstm layer of the predictor\n",
        "input_dim=1024 \n",
        "num_predictions=2  #Number of predictions from softmax layer.  In this instance 2 Since we are focusing on jamaica and trinidad\n",
        "encoder_input_shape=(15,1198)  #input shape -> result of concatenating 13 MFCC, fundamental freqeuncy and energy for each frame\n",
        "predictor_input_shape=(1,2) # predictor input shape -> 2 classes we are trying to identify\n",
        "joiner_input_shape=(160,) #num_encoder_dense+num_predictor_dense\n",
        "batch_size=32 #batch size used in training\n",
        "num_encoder_dense=128 #number of cells in the projection/dense layer of the encoder\n",
        "num_predictor_dense=32 # number of cells in the projection/dense layer of the predictor\n"
      ],
      "metadata": {
        "id": "9jHHzEZB0Thb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Data Preprocessing"
      ],
      "metadata": {
        "id": "kemloB3NZlaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount drive"
      ],
      "metadata": {
        "id": "sNdzUgVLvyMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "id": "JQ6eme0KZk3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "786fee26-21b3-4943-a058-e4db5fd2ae74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gdrive/MyDrive/capstone_data\n",
        "audpath='/content/gdrive/MyDrive/capstone_data'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puaBKeKgT6UJ",
        "outputId": "0e0cde6a-3f86-414d-c92b-e6a7b0e1782f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/capstone_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio PreProcessing Util Class"
      ],
      "metadata": {
        "id": "xYQDy2c8aDSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class contains all the code needed for opening, rechannelling, padding or truncating, removing background and finding the mfccs, fundamental frequency and pitch using the librosa and scipy libraries."
      ],
      "metadata": {
        "id": "er-HuS2J8oH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioPreProc:\n",
        "  def open(file):\n",
        "    \"\"\" Opens given audio file returning an numpy numerical array\n",
        "        and the sampling rate of the file\"\"\"\n",
        "    arr, sr = lb.load(file,mono=True,sr=44100) # sampling rate of 44100 hz \n",
        "    return (arr, sr) #returns a numpy array and the sample rate\n",
        "\n",
        "  @staticmethod\n",
        "  def rechannel(aud, new_channel=1):\n",
        "    \"\"\"\n",
        "    Converts the given audio to the given number of channels.  \n",
        "    In this case it would convert any stereo audio (2 channels)\n",
        "    to mono audio (1 channel)\n",
        "    \"\"\"\"\n",
        "    sig, sr = aud\n",
        "    #if (sig.shape[0] == new_channel):\n",
        "    if(len(sig.shape)==new_channel): #changing as libroase says (n,) is mono bu (2,n) is stereo\n",
        "      return aud\n",
        "    else:\n",
        "      # Convert from mono to stereo by duplicating the first channel\n",
        "      resig=np.array([sig,sig])\n",
        "    return (resig, sr)\n",
        "\n",
        "  @staticmethod\n",
        "  def remove_silence(aud):\n",
        "    \"\"\"\n",
        "    Removes all silence from the given audio array\n",
        "    \"\"\"\n",
        "    sig,sr=aud\n",
        "    print(\"Reg audio \",sig)\n",
        "    clips = lb.effects.split(sig, top_db=10)\n",
        "    wav_data = []\n",
        "    for c in clips:\n",
        "        data = sig[c[0]:c[1]]\n",
        "        wav_data.append(data)\n",
        "    print(\"Silenced audio \",wav_data)\n",
        "    return (wav_data, sr)\n",
        "\n",
        "  @staticmethod\n",
        "  def pad_trunc(aud,max_ms):\n",
        "    \"\"\"\"\n",
        "    Pads the given audio with zeros to the desired length in milliseconds \n",
        "    or shortens the audio signal to the desired length.\n",
        "    \"\"\"\n",
        "    sig,sr = aud\n",
        "    sig_len=round(lb.get_duration(sig,sr=sr)*1000)\n",
        "    max_len = sr//1000 * max_ms\n",
        "    #print(max_len)\n",
        "\n",
        "    if (sig_len > max_len):\n",
        "      # Truncate the signal to the given length\n",
        "      resig = sig[:,:max_len]\n",
        "      #print(\"SIG\",sig)\n",
        "    elif (sig_len < max_len):\n",
        "      resig=lb.util.fix_length(sig, size=max_len)\n",
        "    return (resig,sr)\n",
        "\n",
        "  @staticmethod\n",
        "  def remove_background(aud):\n",
        "    \"\"\"\n",
        "    Attempts to lessen the background noise of audio in order to make\n",
        "    foreground features more obvious.\n",
        "    \"\"\"\n",
        "    sig,sr=aud\n",
        "    b,a = signal.butter(10, 2000/(sr/2), btype='highpass')\n",
        "    sig = signal.lfilter(b,a,sig)\n",
        "    return (sig,sr)\n",
        "\n",
        "  def get_fundamental_freq(aud,hop):\n",
        "    \"\"\"\"\n",
        "    Calculates the fundamental frequency (pitch) of a given audio signal\n",
        "    based on the hop length.  Normalises the values before it is returned.\n",
        "    \"\"\"\n",
        "    sig,sr=aud\n",
        "    f0 = lb.yin(sig, sr = sr, fmin = lb.note_to_hz('C2'), fmax= lb.note_to_hz('C7'),hop_length=hop)\n",
        "    norm_f0=np.linalg.norm(f0)\n",
        "    f0=f0/norm_f0\n",
        "    return f0\n",
        "\n",
        "  def get_energy(aud,hop):\n",
        "    \"\"\"\n",
        "    Calculates the magnitude of a signal for each frame based on the given hop length.\n",
        "    \"\"\"\n",
        "    sig,sr=aud\n",
        "    energy=lb.feature.rms(sig, hop_length=hop, center=True)\n",
        "    return energy\n",
        "\n",
        "  def get_mfccs(sig,num_mfccs=13,window_length=0.02):\n",
        "    \"\"\"\n",
        "    Calculates mffc coefficients for each frame given a window length in seconds.\n",
        "    Return the calculate hop length and mfccs in the shape (num_mffcs,1198)\n",
        "    \"\"\"\n",
        "    aud,sr=sig\n",
        "    n_fft = int(sr * window_length)   # window length: 0.02 s\n",
        "    #print(n_fft,n_fft//2)\n",
        "    hop_length = n_fft // 2  \n",
        "    mfccs = lb.feature.mfcc(aud, sr=sr, n_mfcc=num_mfccs, hop_length=hop_length, n_fft=n_fft)\n",
        "    norm_mfcc=np.linalg.norm(mfccs)\n",
        "    mfccs=mfccs/norm_mfcc\n",
        "    return mfccs,hop_length\n",
        "\n",
        "  def get_formant_freq(sig):\n",
        "    \"\"\"\n",
        "    Calculates formant frequency based on given signal.\n",
        "    \"\"\"\n",
        "    aud,sr=sig\n",
        "    A = librosa.core.lpc(aud,4)\n",
        "    rts = np.roots(A)\n",
        "    rts = rts[np.imag(rts) >= 0]\n",
        "    angz = np.arctan2(np.imag(rts), np.real(rts))\n",
        "    frqs = angz * sr / (2 *  np.pi)\n",
        "    frqs.sort()\n",
        "    norm_formant=np.linalg.norm(frqs)\n",
        "    fin=frqs/norm_formant\n",
        "    makeup=(1198//len(fin))+1\n",
        "    fin=np.tile(fin,makeup)\n",
        "    return fin[:1198]\n",
        " \n",
        "    "
      ],
      "metadata": {
        "id": "8MdDWPj9aC5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "0953df3d-cf46-4a80-943c-6259bfecf0dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-0bd1dd6cdf6e>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    \"\"\"\"\u001b[0m\n\u001b[0m        \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Generator Class"
      ],
      "metadata": {
        "id": "MU7RxIydHpOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Created so that data is loaded in batches instead of all at once."
      ],
      "metadata": {
        "id": "V0DoYxrLHuwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(Sequence):\n",
        "    \"\"\"Generates data for Keras\n",
        "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
        "    \"\"\"\n",
        "    def __init__(self, filenames, audlabels, audio_path, \n",
        "                 to_fit=True, batch_size=batch_size, dim=encoder_input_shape,\n",
        "                 n_channels=1, n_classes=2, shuffle=True):\n",
        "        \"\"\"Initialization\n",
        "        :param filenames: list of audio file names\n",
        "        :param audlabels: respective accent_id labels for filenames\n",
        "        :param audio_path: path to audio locations\n",
        "        :param to_fit: True to return X and y, False to return X only\n",
        "        :param batch_size: batch size at each iteration\n",
        "        :param dim: tuple indicating audio dimension\n",
        "        :param n_channels: number of audio channels\n",
        "        :param n_classes: number of output masks\n",
        "        :param shuffle: True to shuffle label indexes after every epoch\n",
        "        \"\"\"\n",
        "        self.audlabels = audlabels\n",
        "        self.filenames = filenames\n",
        "        self.audio_path = audio_path\n",
        "        self.to_fit = to_fit\n",
        "        self.batch_size = batch_size\n",
        "        self.dim = dim\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.target_dim=(1,n_classes)\n",
        "        self.duration = 12000 #length in milliseconds   #rn at 10 seconds\n",
        "        self.shift_pct = 0.4\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Denotes the number of batches per epoch\n",
        "        :return: number of batches per epoch\n",
        "        \"\"\"\n",
        "        return int(np.floor(len(self.filenames) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generate one batch of data\n",
        "        :param index: index of the batch\n",
        "        :return: X and y when fitting. X only when predicting\n",
        "        \"\"\"\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.filenames[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X,y,target= self._generate_data(list_IDs_temp)\n",
        "\n",
        "        if self.to_fit:\n",
        "            return [X, target],y\n",
        "        else:\n",
        "            return X\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\n",
        "        \"\"\"\n",
        "        self.indexes = np.arange(len(self.filenames))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def _generate_data(self, list_IDs_temp):\n",
        "        \"\"\"\n",
        "        Converts every audio in the given array using the AudioPreProc class.\n",
        "        Also creates and returns a target array with different probability matrixes\n",
        "        to train the predictor and an array with the labels for each file in the dataset.\n",
        "\n",
        "        \"\"\"\n",
        "        # Initialization       \n",
        "        X = np.empty((self.batch_size, *self.dim),dtype=float)\n",
        "        y = np.empty((self.batch_size), dtype=float)\n",
        "        target=np.empty((self.batch_size, *self.target_dim),dtype=float)\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            (aud,sr) = AudioPreProc.open(ID)   \n",
        "            tot=AudioPreProc.remove_background((aud,sr))\n",
        "            tot=AudioPreProc.pad_trunc(tot,12000)\n",
        "            \n",
        "            mfcc,hop=AudioPreProc.get_mfccs(tot)\n",
        "            #print('MFCC',mfcc.shape)\n",
        "            #formant=AudioPreProc.get_formant_freq(tot)\n",
        "            #formant=formant.reshape((1,formant.shape[0]))\n",
        "            f0=AudioPreProc.get_fundamental_freq(tot,hop)\n",
        "            f0=f0.reshape((1,f0.shape[0]))\n",
        "            #print('FF',f0.shape)\n",
        "            energy=AudioPreProc.get_energy(tot,hop)\n",
        "            #print('energy',energy.shape)\n",
        "\n",
        "            fin=np.concatenate([mfcc,f0,energy])\n",
        "            X[i,]=fin\n",
        "            y[i] = self.audlabels[i]  \n",
        "           \n",
        "            #target[i,]=np.array([0.1 if self.audlabels[i]!=x else 0.9 for x in range(self.n_classes)]).reshape((1,self.n_classes))\n",
        "            target[i,]=np.array([np.random.uniform(low=0.1,high=0.5) if self.audlabels[i]!=x else np.random.uniform(low=0.5,high=0.1) for x in range(self.n_classes)]).reshape((1,self.n_classes))\n",
        "        return X,y,target\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "ephzlGoSPHng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data and Data Generator Objects"
      ],
      "metadata": {
        "id": "SDMK4vosYsE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('labels3.csv',header=None)\n",
        "df.columns=['path','accent'] #renaming columns\n",
        "\n",
        "lst=[0 if x==\"Jamaican\" else 1 for x in df['accent']] #assigning class ids to text\n",
        "df['accent_id']=lst\n",
        "df = df.drop(columns=['accent'])\n",
        "trin=df[:521]\n",
        "jam=df[521:]\n",
        "\n",
        "jam=jam.sample(521)  # random under sampling of jamaican data to get equal data from jamaican and trinidadian data sets\n",
        "\n",
        "new_df=pd.concat([trin,jam],axis=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "xM5J14uBvvjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting data into training, testing and validation sets"
      ],
      "metadata": {
        "id": "XXulI-jKKgjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train, X_valtest, y_train, y_valtest = train_test_split(new_df['path'], new_df['accent_id'],test_size=0.20,random_state=21)\n",
        "X_val,X_test, y_val,y_test=train_test_split(X_valtest, y_valtest,test_size=0.50,random_state=22) # test size is 0.5 since we want vlength of val and test to be equal\n"
      ],
      "metadata": {
        "id": "1rtLRBnCKgMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Converting everything to numpy arrays so hat indexes function in data generator works\n",
        "X_train=np.array(X_train)\n",
        "X_val=np.array(X_val)\n",
        "X_test=np.array(X_test)\n",
        "y_train=np.array(y_train)\n",
        "y_val=np.array(y_val)\n",
        "y_test=np.array(y_test)\n",
        "\n",
        "#print(X_test)\n"
      ],
      "metadata": {
        "id": "bAjwNvwfHHLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating validation and training data generators\n"
      ],
      "metadata": {
        "id": "25HqVGGgbFAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_generator=DataGenerator(X_train,y_train,audpath)\n",
        "validation_generator=DataGenerator(X_val,y_val,audpath)\n",
        "#prediction_generator=DataGenerator(X_test,y_test,audpath,to_fit=False,shuffle=False,batch_size=1)\n"
      ],
      "metadata": {
        "id": "yE_2d3cobLX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating RNNT Model"
      ],
      "metadata": {
        "id": "SULQxiRZjes4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Class"
      ],
      "metadata": {
        "id": "Bu-4Mk9ssu-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Encoder(keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  Subclass of keras.layers.Layer.  \n",
        "  Consists of 3 LSTM layers with 512 cells each  and a projection/dense layer with 128 cells\n",
        "  \"\"\"\n",
        "  def __init__(self,num_hidden_encoder,encoder_input_shape,num_encoder_dense):\n",
        "    \"\"\"Initialization\n",
        "        :param num_hidden_encoder: the number of cells in each LSTM layers\n",
        "        :param encoder_input_shape: the shape of data to be used as input\n",
        "        :param num_encoder_dense: the number of cells in the dense layer.\n",
        "    \"\"\"\n",
        "    super(Encoder,self).__init__()\n",
        "    self.num_hidden_e=num_hidden_encoder\n",
        "    self.input_shape_e=encoder_input_shape\n",
        "    self.num_enc_dense=num_encoder_dense\n",
        "    self.e_input=InputLayer(input_shape=self.input_shape_e,name=\"encoder_input\",dtype=tf.float64)\n",
        "    self.e_masking=layers.Masking(mask_value=0.0,input_shape=self.input_shape_e)\n",
        "    self.l1= LSTM(self.num_hidden_e,return_sequences=True,dropout=0.1,name=\"e_lstm1\")\n",
        "    self.l2= LSTM(self.num_hidden_e,return_sequences=True,dropout=0.1,name=\"e_lstm2\")\n",
        "    #self.l3= LSTM(self.num_hidden_e,return_sequences=True,dropout=0.1,name=\"e_lstm3\")\n",
        "    self.l4= LSTM(self.num_hidden_e,return_sequences=True,dropout=0.1,name=\"e_lstm4\")\n",
        "    self.e_dense=Dense(self.num_enc_dense) \n",
        "   \n",
        "\n",
        "  def call(self, input, is_sequence=False):\n",
        "    \"\"\"\n",
        "    Passes input from input layer to masking to the lstm layers\n",
        "    and then the dense layer.  If is_sequence is true then it will return\n",
        "    the results for every timestep in the input while if false it will only\n",
        "    return the result for the last timestep.\n",
        "    \"\"\"\n",
        "    i= self.e_input(input)\n",
        "    mask_out=self.e_masking(i)\n",
        "    output=self.l1(mask_out)\n",
        "    output=self.l2(output)\n",
        "    #output=self.l3(output)\n",
        "    output=self.l4(output)\n",
        "    fin=self.e_dense(output)\n",
        "    if is_sequence:\n",
        "      return fin\n",
        "    else:\n",
        "      return fin[:,-1]\n",
        "\n",
        "    \n",
        "  def initialize_states(self, batch_size=32):   \n",
        "    \"\"\"\n",
        "    Creates zero tensor of shape (batch_size,num_hidden_encoder).\n",
        "    Previously used to intialise lstm encoder states.\n",
        "    \"\"\" \n",
        "    return (tf.zeros([batch_size, self.num_hidden_e],tf.float32),\n",
        "                tf.zeros([batch_size, self.num_hidden_e], tf.float32))\n",
        "    \n",
        "  def get_config(self):\n",
        "    config = super(Encoder, self).get_config()\n",
        "    #config={}\n",
        "    config.update({\"num_hidden_encoder\": self.num_hidden_e})\n",
        "    config.update({\"encoder_input_shape\": self.input_shape_e})\n",
        "    return config"
      ],
      "metadata": {
        "id": "cyAPp_4Rpil7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predictor Class"
      ],
      "metadata": {
        "id": "nXAcJs_ustPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Single_Step_Predictor(keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  Subclass of keras.layers.Layer\n",
        "  The predictor class of the RNN-T model. Consists of 1 lstm layer with 64 units\n",
        "  and a dense projection layer with 32 units.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_hidden_predictor,predictor_input_shape,num_predictor_dense):\n",
        "    \"\"\"Initialization\n",
        "        :param num_hidden_predictor: the number of cells in each LSTM layer\n",
        "        :param predictor_input_shape: the shape of data to be used as input\n",
        "        :param num_predictor_dense: the number of cells in the dense layer.\n",
        "    \"\"\"\n",
        "    super(Single_Step_Predictor,self).__init__()\n",
        "    self.num_hidden_p=num_hidden_predictor\n",
        "    self.num_pred_dense=num_predictor_dense\n",
        "    self.input_shape_p=predictor_input_shape\n",
        "    self.p_input=InputLayer(input_shape=self.input_shape_p,name=\"input_predictor\")\n",
        "    self.pl1= LSTM(self.num_hidden_p,return_state=True,dropout=0.1,name=\"p_lstm\")\n",
        "    #self.pl2= LSTM(self.num_hidden_p,return_state=True, dropout=0.2)\n",
        "    self.p_dense=Dense(self.num_pred_dense)\n",
        "   \n",
        "\n",
        "  def call(self, input, state_h,state_c):\n",
        "    \"\"\"\n",
        "    Call method of the predictor. Accepts a probability matrix, hidden state and \n",
        "    a cell state and uses the hiddden and cell state as the initial state of the\n",
        "    lstm layer.\n",
        "    \"\"\"\n",
        "    i=self.p_input(input)\n",
        "    output,hidd_state, cell_state=self.pl1(i,initial_state=[state_h,state_c])\n",
        "    fin=self.p_dense(output)\n",
        "\n",
        "    return fin, hidd_state, cell_state\n",
        "\n",
        "  def initialize_states(self, batch_size=4):\n",
        "    \"\"\"\n",
        "    Creates a zero tensor to act as the initial states of the lstm layers \n",
        "    based on the given batch size and the number of cells in the lstm layer.\n",
        "    \"\"\"    \n",
        "    return (tf.zeros([batch_size, self.num_hidden_p],tf.float32),\n",
        "                tf.zeros([batch_size, self.num_hidden_p],tf.float32))\n",
        "  \n",
        "  def get_config(self):\n",
        "    config = super(Single_Step_Predictor, self).get_config()\n",
        "    #config={}\n",
        "    config.update({\"num_hidden_predictor\": self.num_hidden_p})\n",
        "    config.update({\"predictor_input_shape\": self.input_shape_p})\n",
        "    return config\n",
        "\n"
      ],
      "metadata": {
        "id": "sBhnoY0Oss0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Joiner Class"
      ],
      "metadata": {
        "id": "v4Je0_NdzKSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@tf.keras.utils.register_keras_serializable(package='Custom', name=None)\n",
        "class Joiner(keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  Subclass of keras.layers.Layer\n",
        "  The joiner class of the RNN-T model. Consists of 1 Dense layer with 64 units\n",
        "  and a dense layer with softmax activation with 2 units (the number of classes).\n",
        "  \"\"\"\n",
        "  def __init__(self, num_hidden_joiner, joiner_input_shape, num_predictions):\n",
        "    \"\"\"Initialization\n",
        "        :param num_hidden_joinerr: the number of cells in the regular Dense layer\n",
        "        :param joiner_input_shape: the shape of data to be used as input\n",
        "        :param num_predictions: the number of cells in the dense layer with softmax activation.\n",
        "    \"\"\"\n",
        "    super(Joiner,self).__init__()\n",
        "    self.num_hidden_j=num_hidden_joiner\n",
        "    self.input_shape_j=joiner_input_shape\n",
        "    self.num_predictions=num_predictions\n",
        "    self.j_input=InputLayer(input_shape=self.input_shape_j,name=\"joiner_input\")\n",
        "    self.d1= Dense(self.num_hidden_j,activation=None,name=\"joiner_dense\",input_shape=self.input_shape_j)\n",
        "    self.d2=Dense(self.num_predictions,activation='softmax')\n",
        "    \n",
        "  \n",
        "  def call(self, input):\n",
        "    \"\"\"\n",
        "    Simple layer call. Passes input through the input layer, then the first\n",
        "    dense layer then the softmax activation layer.  Returns the result of the\n",
        "    softmax activation layer.\n",
        "    \"\"\"\n",
        "    i=self.j_input(input)\n",
        "    intermediate=self.d1(i)\n",
        "    fin= self.d2(intermediate)\n",
        "\n",
        "    return fin\n",
        "\n",
        "  def get_config(self):\n",
        "\n",
        "    config = super(Joiner, self).get_config()\n",
        "    config.update({\"num_hidden_joiner\": self.num_hidden_j})\n",
        "    config.update({\"joiner_input_shape\": self.input_shape_j})\n",
        "    config.update({\"num_predictions\":self.num_predictions})\n",
        "    return config\n"
      ],
      "metadata": {
        "id": "u4QgV4uozLvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNNT CLass"
      ],
      "metadata": {
        "id": "UCkxRw8arQgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class RNNT3(keras.Model):\n",
        "  \"\"\"\n",
        "  A model which consists of an encoder, and a predictor and a joiner.\n",
        "  For each frame the encoder processes the input while the predictor \n",
        "  processes the previous prediction.  The output of encoder and the output\n",
        "  of the predictor is then concatenated and used as input to the joiner layer\n",
        "  which will produce a probability matrix.  This probability matrix will\n",
        "  then be used as input for the predictor on the next time step.\n",
        "  \"\"\"\n",
        "  def __init__(self, num_hidden_encoder, num_hidden_predictor, num_hidden_joiner, num_predictions, joiner_input_shape,predictor_input_shape,encoder_input_shape,num_encoder_dense,num_predictor_dense):\n",
        "    \"\"\"Initialization\n",
        "        :param num_hidden_joiner: the number of cells in the joiner's regular Dense layer.\n",
        "        :param joiner_input_shape: the shape of data to be used as input to the joiner.\n",
        "        :param num_predictions: the number of cells in the joiner's dense layer with softmax activation.\n",
        "        :param num_hidden_predictor: the number of cells in the predictor's LSTM layer.\n",
        "        :param predictor_input_shape: the shape of data to be used as input to the predictor.\n",
        "        :param num_predictor_dense: the number of cells in the predictor's dense layer.\n",
        "        :param num_hidden_encoder: the number of cells in each of the encoder's LSTM layers.\n",
        "        :param encoder_input_shape: the shape of data to be used as input to the encoder.\n",
        "        :param num_encoder_dense: the number of cells in the encoder's dense layer.\n",
        "    \"\"\"\n",
        "    \n",
        "    super(RNNT3,self).__init__()\n",
        "    self.num_hidden_encoder=num_hidden_encoder\n",
        "    self.num_hidden_predictor=num_hidden_predictor\n",
        "    self.num_hidden_joiner=num_hidden_joiner\n",
        "    self.num_predictions=num_predictions\n",
        "    self.num_enc_dense=num_encoder_dense\n",
        "    self.num_pred_dense=num_predictor_dense\n",
        "    self.joiner_input_shape=joiner_input_shape\n",
        "    self.predictor_input_shape=predictor_input_shape\n",
        "    self.encoder_input_shape=encoder_input_shape\n",
        "    self.encoder=Encoder(self.num_hidden_encoder,self.encoder_input_shape,num_encoder_dense)\n",
        "    self.predictor=Single_Step_Predictor(self.num_hidden_predictor,self.predictor_input_shape,num_predictor_dense)\n",
        "    self.joiner=Joiner(self.num_hidden_joiner, self.joiner_input_shape, self.num_predictions)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    \"\"\"\n",
        "    Used for training only.\n",
        "    States of the predictor is initialised.  The input sequence\n",
        "    is broken into the features extracted from the audio file\n",
        "    and a target array containing probability matrixes corresponding to an accent.\n",
        "    The encoder is given the features as input while the predictor is give the target array.\n",
        "    Output for the last timestep in the encoder and the predictor is concatenated and used\n",
        "    as input to the joiner.\n",
        "\n",
        "    Previously the call method was implemented in a way similar to make_prediction,\n",
        "    but validation accuracy still stagnated at 56.25%\n",
        "    \"\"\"\n",
        "    pred_init=self.predictor.initialize_states(32)\n",
        "    hid,cell=pred_init[0],pred_init[1]\n",
        "    encoder_out=self.encoder(inputs[0],is_sequence=False) \n",
        "    pred_out,hid,cell=self.predictor(inputs[1],hid,cell)  ## only expecting 1 value from y but we need two\n",
        "    joiner_input=tf.concat([encoder_out,pred_out],axis=1)\n",
        "    fin=self.joiner(joiner_input)\n",
        "    return fin\n",
        "\n",
        " \n",
        "  def one_step_decode(self,encoder_out,timestep):\n",
        "    \"\"\"\n",
        "    For every timestep in the output of the encoder,\n",
        "    the predictor is called, the output of \n",
        "    the predictor and the output of the encoder for that timestepp\n",
        "    is given to the joiner and a prediction is made which is then \n",
        "    used by the predictor in the calculation for the next frame.\n",
        "\n",
        "    Returns the last prediction made by the joiner (i.e.\n",
        "    the prediction made using the last timestep of the encoder output) \n",
        "    \"\"\"\n",
        "    prev_predict=[]\n",
        "    pred_init=self.predictor.initialize_states(1)\n",
        "    state_h=pred_init[0]\n",
        "    state_c=pred_init[1]\n",
        "    pred_out=np.array([[0.5,0.5]]).reshape(self.predictor_input_shape)\n",
        "    for step in range(self.encoder_input_shape[0]):\n",
        "        pred_out, state_h, state_c = self.predictor(tf.reshape(pred_out,[1,1,2]), state_h,state_c,training=False)\n",
        "        joiner_input=tf.concat([tf.reshape(encoder_out[step],[1,self.num_enc_dense]),pred_out],axis=1) ##change to num_hidden_encoder\n",
        "        pred_out=self.joiner(joiner_input,training=False)\n",
        "        prev_predict=pred_out\n",
        "    return tf.convert_to_tensor(prev_predict)\n",
        "\n",
        "  @tf.function\n",
        "  def make_prediction(self,input,batch_size):\n",
        "    \"\"\"\n",
        "    Used in validation. Propagates audio through the encoder,\n",
        "    predictor and joinger for every timestep by batch.\n",
        "\n",
        "    For every timestep in the output of the encoder,\n",
        "    the predictor is called, the output of \n",
        "    the predictor and the output of the encoder for that timestepp\n",
        "    is given to the joiner and a prediction is made which is then \n",
        "    used by the predictor in the calculation for the next frame.\n",
        "\n",
        "    Returns two arrays containing the classified accent labels and the probability matrix \n",
        "    for each sample in the batch.\n",
        "    \"\"\"\n",
        "    prev_predict=tf.TensorArray(tf.float32,size=batch_size)\n",
        "    np_output=tf.TensorArray(tf.int64, size=batch_size)\n",
        "\n",
        "    argmax_outputs=tf.TensorArray(tf.float32, size=batch_size)\n",
        "\n",
        "    encoder_outputs= self.encoder(input, is_sequence=True,training=False)\n",
        "\n",
        "    ins=tf.convert_to_tensor(encoder_outputs)\n",
        "\n",
        "    timesteps = ins.shape[1]\n",
        "    pred_init=self.predictor.initialize_states(batch_size)\n",
        "    state_h=pred_init[0]\n",
        "    state_c=pred_init[1]\n",
        "    pred_out=tf.reshape([0.5 for i in range(batch_size*self.num_predictions)],[batch_size,1,num_predictions])\n",
        "    for step in range(self.encoder_input_shape[0]):\n",
        "      pred_out, state_h, state_c = self.predictor(tf.reshape(pred_out,[batch_size,1,2]), state_h,state_c,training=False)\n",
        "      joiner_input=tf.concat([tf.reshape(ins[:,step],[batch_size,self.num_enc_dense]),pred_out],axis=1) ##change to num_hidden_encoder\n",
        "      pred_out=self.joiner(joiner_input,training=False)\n",
        "      prev_predict=pred_out\n",
        "\n",
        "    np_output=tf.math.argmax(prev_predict)\n",
        "    argmax_output=prev_predict\n",
        "\n",
        "    return np_output, argmax_output\n",
        "\n",
        "  def predict_val(self,input,batch_size):\n",
        "    \"\"\" Calls one_step_decode for every audio in a batch.\n",
        "    Used to make predictions, outside of training and validation.\n",
        "    Replaces model.predict()\n",
        "    \"\"\"\n",
        "    np_output=[]\n",
        "    argmax_outputs=[]\n",
        "    encoder_outputs= self.encoder(input, is_sequence=True,training=False)\n",
        "    ins=tf.convert_to_tensor(encoder_outputs)\n",
        "    timesteps = ins.shape[1]\n",
        "    print(ins.shape)\n",
        "    for i in range(batch_size): # for every input in the batch\n",
        "        decoded_seq = self.one_step_decode(ins[i], timesteps)\n",
        "        np_output.append(np.argmax(decoded_seq))\n",
        "    \n",
        "        argmax_outputs.append(decoded_seq)\n",
        "    return np_output,argmax_outputs\n",
        "\n",
        "  @tf.function\n",
        "  def test_step(self, data):\n",
        "    \"\"\"\n",
        "    Validation step. Calculates losses based on the \n",
        "    results of make_prediction.\n",
        "    \"\"\"\n",
        "    # Unpack the data\n",
        "    X, y = data\n",
        "    # Compute predictions\n",
        "    #y_pred_not_np,raw = self(x, training=False)\n",
        "    y_pred_not_np, raw = self.make_prediction(X[0],32)\n",
        "    raw=tf.reshape(raw,[32,2])\n",
        "    # Updates the metrics tracking the loss\n",
        "    self.compiled_loss(tf.convert_to_tensor(y),tf.convert_to_tensor(raw) ,regularization_losses=self.losses)\n",
        "    # Update the metrics.\n",
        "    self.compiled_metrics.update_state(tf.convert_to_tensor(y),tf.convert_to_tensor(raw))\n",
        "    # Return a dict mapping metric names to current value.\n",
        "    # Note that it will include the loss (tracked in self.metrics).\n",
        "    return {m.name: m.result() for m in self.metrics}\n",
        "  \n",
        "  \n",
        "  \"\"\"def get_config(self):\n",
        "    #config = super(RNNT3, self).get_config()\n",
        "    config={}\n",
        "    config.update({\"num_hidden_encoder\": self.num_hidden_encoder})\n",
        "    config.update({\"num_hidden_predictor\": self.num_hidden_predictor})\n",
        "    config.update({\"num_hidden_joiner\": self.num_hidden_joiner})\n",
        "    config.update({\"joiner_input_shape\": self.joiner_input_shape})\n",
        "    config.update({\"predictor_input_shape\": self.predictor_input_shape})\n",
        "    config.update({\"encoder_input_shape\": self.encoder_input_shape})\n",
        "    config.update({\"num_predictions\":self.num_predictions})\n",
        "    return config\"\"\"\n"
      ],
      "metadata": {
        "id": "T_IMPJEa1Fc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compiling and training Model"
      ],
      "metadata": {
        "id": "o4CWYESRAuX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rnn3=RNNT3(num_hidden_encoder, num_hidden_predictor, num_hidden_joiner, num_predictions, joiner_input_shape,predictor_input_shape,encoder_input_shape,num_encoder_dense,num_predictor_dense)\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n",
        "                                        mode =\"min\", patience = 5, \n",
        "                                        restore_best_weights = True)\n",
        "\n",
        "checkpoint_path = \"checkpoints/cp_teststep_with512_encoder-epoch{epoch:04d}_acc{val_accuracy:.3f}.ckpt\"\n",
        "\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    monitor='val_loss',\n",
        "    verbose=1, \n",
        "    save_best_only=True,\n",
        "    mode='min',\n",
        "    filepath=checkpoint_path,   \n",
        "    save_weights_only=True,\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "pxSUwXCvAxy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "rnn3.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "6PsLk031r7W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training rnnt3"
      ],
      "metadata": {
        "id": "GrTCSOfDD7pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hist=rnn3.fit(training_generator, validation_data=validation_generator,epochs=10,callbacks =[earlystopping,cp_callback],verbose=1) \n",
        "#hist=rnn3.fit(training_generator,epochs=1,verbose=1)\n"
      ],
      "metadata": {
        "id": "YQ8FMg6NEAE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F3rlpvktTVTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction_data(X_test):\n",
        " \n",
        "  batch_size=len(X_test)\n",
        "  X = np.empty((batch_size, *encoder_input_shape),dtype=float)\n",
        "  print('Batch size: ',batch_size)\n",
        "  for i in range(batch_size):\n",
        "    (aud,sr) = AudioPreProc.open(X_test[i])   \n",
        "    tot = AudioPreProc.rechannel((aud,sr), 1)\n",
        "    tot=AudioPreProc.pad_trunc(tot,12000)\n",
        "            \n",
        "    mfcc,hop=AudioPreProc.get_mfccs(tot)\n",
        "    #print('MFCC',mfcc.shape)\n",
        "    f0=AudioPreProc.get_fundamental_freq(tot,hop)\n",
        "    f0=f0.reshape((1,f0.shape[0]))\n",
        "    #print('FF',f0.shape)\n",
        "    energy=AudioPreProc.get_energy(tot,hop)\n",
        "    #print('energy',energy.shape)\n",
        "\n",
        "    fin=np.concatenate([mfcc,f0,energy])\n",
        "    X[i,]=fin\n",
        "    \n",
        "  return batch_size, X\n",
        "\n",
        "\n",
        "batchsize,Xt=get_prediction_data(X_test[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI6-OCBtZY3L",
        "outputId": "82bc8614-714e-4501-9acb-f0fc2c95173d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size:  10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#rnn3.load_weights('checkpoints/checkpoints/cpc-epoch0005--loss0.6417.ckpt') #cpc has 256 enc, 4 pcells, joiner input of 260, overall prediction of 49% good\n",
        "#mix of jam and trinidadian 256 denser encoder, 4 dense predictor\n",
        "\n",
        "#rnn3.load_weights('checkpoints/cp_teststep_with64_predictor-epoch0001_acc0.562.ckpt')\n",
        "pred,non_argmax=rnn3.predict_val(Xt,10)"
      ],
      "metadata": {
        "id": "7YT-Pl_JTUdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "998d23f6-10b2-444d-da19-ffc016e1cb77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 15, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy Rate Calculation"
      ],
      "metadata": {
        "id": "jworHpzd3ZRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pred=np.array(pred)\n",
        "count=0\n",
        "for i in range(len(pred)):\n",
        "  if pred[i]==y_test[i]:\n",
        "    count+=1\n",
        "print(count/len(pred)*100)\n",
        "\n",
        "print(np.array(pred))\n",
        "print(y_test)"
      ],
      "metadata": {
        "id": "bo-CIBHyZF50"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}